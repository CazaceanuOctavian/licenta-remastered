{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/nerd-for-tech/predicting-price-of-smart-phones-by-technical-specs-random-forest-logistic-regression-48ddc0cdeb0c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('datasets/evomag_2024_11_13.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_smartphones(df):\n",
    "    \"\"\"\n",
    "    Extract all products that have \"Smartphone\": \"Da\" in their specifications column.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing product information with a 'specifications' column\n",
    "                          that contains dictionaries with product specs\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: A new DataFrame containing only smartphone products\n",
    "    \"\"\"\n",
    "    # Create a mask to filter products where specifications contains \"Smartphone\": \"Da\"\n",
    "    smartphone_mask = df['specifications'].apply(\n",
    "        lambda specs: isinstance(specs, dict) and specs.get('Smartphone') == 'Da'\n",
    "    )\n",
    "    \n",
    "    # Apply the mask to get only smartphone products\n",
    "    smartphones_df = df[smartphone_mask].copy()\n",
    "    \n",
    "    return smartphones_df\n",
    "\n",
    "# Example usage:\n",
    "# smartphones = extract_smartphones(df)\n",
    "# print(f\"Found {len(smartphones)} smartphones out of {len(df)} total products\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "def flatten_json_column(df, json_column):\n",
    "    \"\"\"\n",
    "    Flatten a JSON column in a DataFrame so that the fields become separate columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame containing the JSON column to flatten\n",
    "    json_column : str\n",
    "        The name of the column containing the JSON data to flatten\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        A new DataFrame with the JSON column flattened into separate columns\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Check if the JSON column exists in the DataFrame\n",
    "    if json_column not in result_df.columns:\n",
    "        raise ValueError(f\"Column '{json_column}' not found in DataFrame\")\n",
    "    \n",
    "    # Normalize the JSON column\n",
    "    try:\n",
    "        # Handle cases where some rows might have None/NaN values in the JSON column\n",
    "        mask = result_df[json_column].notna()\n",
    "        \n",
    "        if mask.any():\n",
    "            # Apply json_normalize only to rows that have valid JSON\n",
    "            normalized_df = json_normalize(result_df.loc[mask, json_column])\n",
    "            \n",
    "            # Drop the original JSON column from the result\n",
    "            result_subset = result_df.loc[mask].drop(json_column, axis=1)\n",
    "            \n",
    "            # Combine the original DataFrame (minus the JSON column) with the normalized data\n",
    "            flattened_subset = pd.concat([result_subset.reset_index(drop=True), \n",
    "                                          normalized_df.reset_index(drop=True)], \n",
    "                                         axis=1)\n",
    "            \n",
    "            # Merge back with rows that had None/NaN values\n",
    "            if (~mask).any():\n",
    "                result_df = pd.concat([flattened_subset, \n",
    "                                       result_df.loc[~mask]]).sort_index()\n",
    "            else:\n",
    "                result_df = flattened_subset\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error flattening JSON column: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luam doar telefoanele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smartphone = extract_smartphones(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smartphone.head()\n",
    "df_smartphone.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smartphone_normalised = flatten_json_column(df_smartphone, 'specifications')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smartphone_normalised.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smartphone_normalised.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smartphone_normalised.count().sort_values(axis=0).to_csv(\"count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_smartphone_normalised\n",
    "y = df_smartphone_normalised['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_training = pd.DataFrame()\n",
    "df_model_training['5G'] = df_smartphone_normalised['5G']\n",
    "df_model_training['4G'] = df_smartphone_normalised['4G']\n",
    "\n",
    "df_model_training[['resolution width', 'resolution height']] = df_smartphone_normalised['Rezolutie maxima (px)'].str.split(' x ', expand=True)\n",
    "\n",
    "df_model_training['Diagonala'] = df_smartphone_normalised['Diagonala (inch)']\n",
    "df_model_training['Numar nuclee'] = df_smartphone_normalised['Numar nuclee']\n",
    "df_model_training['Memorie RAM'] = df_smartphone_normalised['Memorie RAM']\n",
    "df_model_training['Incarcare Wireless'] = df_smartphone_normalised['Incarcare Wireless']\n",
    "df_model_training['Capacitate Baterie'] = df_smartphone_normalised['Capacitate'] \n",
    "df_model_training['Dual SIM'] = df_smartphone_normalised['Dual SIM']\n",
    "df_model_training['Manufacturer'] = df_smartphone_normalised['manufacturer']\n",
    "df_model_training['price'] = df_smartphone_normalised['price']\n",
    "\n",
    "df_model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_training['5G'].fillna(0, inplace=True)\n",
    "df_model_training['5G'].replace('Da', 1, inplace=True)\n",
    "df_model_training['5G'].replace('Nu', 0, inplace=True)\n",
    "\n",
    "\n",
    "df_model_training['4G'].fillna(0, inplace=True)\n",
    "df_model_training['4G'].replace('Da', 1, inplace=True)\n",
    "df_model_training['4G'].replace('Nu', 0, inplace=True)\n",
    "\n",
    "\n",
    "df_model_training['resolution width'] = pd.to_numeric(df_model_training['resolution width'], errors='coerce')\n",
    "df_model_training['resolution height'] = pd.to_numeric(df_model_training['resolution height'], errors='coerce')\n",
    "\n",
    "df_model_training['resolution height'].fillna(0, inplace=True)\n",
    "df_model_training['resolution width'].fillna(0, inplace=True)\n",
    "\n",
    "df_model_training['Diagonala'] = pd.to_numeric(df_model_training['Diagonala'], errors='coerce')\n",
    "df_model_training['Diagonala'].fillna(0, inplace=True)\n",
    "\n",
    "df_model_training['Numar nuclee'] = df_model_training['Numar nuclee'].str.split('(').str[0]\n",
    "df_model_training['Numar nuclee'] = pd.to_numeric(df_model_training['Numar nuclee'], errors='coerce')\n",
    "df_model_training['Numar nuclee'].fillna(0, inplace=True)\n",
    "\n",
    "df_model_training['Memorie RAM'] = df_model_training['Memorie RAM'].str.split(' ').str[0]\n",
    "df_model_training['Memorie RAM'] = pd.to_numeric(df_model_training['Memorie RAM'], errors='coerce')\n",
    "df_model_training['Memorie RAM'].fillna(0, inplace=True)\n",
    "\n",
    "df_model_training['Incarcare Wireless'].fillna(0, inplace=True)\n",
    "df_model_training['Incarcare Wireless'].replace('Da', 1, inplace=True)\n",
    "df_model_training['Incarcare Wireless'].replace('Nu', 0, inplace=True)\n",
    "\n",
    "\n",
    "df_model_training['Capacitate Baterie'] = df_model_training['Capacitate Baterie'].str.split(' ').str[0]\n",
    "df_model_training['Capacitate Baterie'] = pd.to_numeric(df_model_training['Capacitate Baterie'], errors='coerce')\n",
    "df_model_training['Capacitate Baterie'].fillna(0, inplace=True)\n",
    "\n",
    "df_model_training['Dual SIM'].fillna(0, inplace=True)\n",
    "df_model_training['Dual SIM'].replace('Da', 1, inplace=True)\n",
    "df_model_training['Dual SIM'].replace('Nu', 0, inplace=True)\n",
    "\n",
    "\n",
    "manufacturers = df_model_training['Manufacturer'].unique()\n",
    "manufacturer_mapping = {manufacturer: i for i, manufacturer in enumerate(sorted(manufacturers))}\n",
    "\n",
    "# Apply the mapping to create a new encoded column\n",
    "df_model_training['Manufacturer'] = df_model_training['Manufacturer'].map(manufacturer_mapping)\n",
    "\n",
    "\n",
    "df_model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "def detect_outliers(df, columns=None, methods=None):\n",
    "    \"\"\"\n",
    "    Detect outliers in specified columns of a DataFrame using various methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame to analyze\n",
    "    columns : list or None\n",
    "        List of column names to check for outliers. If None, all numeric columns are used.\n",
    "    methods : list or None\n",
    "        List of methods to use for outlier detection. Available methods:\n",
    "        - 'zscore': Z-score method (marks values beyond 3 standard deviations)\n",
    "        - 'iqr': Interquartile Range method (marks values below Q1-1.5*IQR or above Q3+1.5*IQR)\n",
    "        - 'percentile': Percentile method (marks values below 1st or above 99th percentile)\n",
    "        - 'dbscan': DBSCAN clustering method (requires scikit-learn)\n",
    "        - 'isolation_forest': Isolation Forest method (requires scikit-learn)\n",
    "        If None, uses ['zscore', 'iqr']\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Dictionary containing outlier information for each column and method\n",
    "    DataFrame: Original DataFrame with outlier flags added\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        # Select only numeric columns\n",
    "        columns = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    \n",
    "    if methods is None:\n",
    "        methods = ['zscore', 'iqr']\n",
    "    \n",
    "    results = {}\n",
    "    df_with_flags = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        # Skip columns with all zeros or binary columns (0/1)\n",
    "        if (df[col] == 0).all() or set(df[col].unique()).issubset({0, 1}):\n",
    "            continue\n",
    "            \n",
    "        # Skip columns with too many missing values\n",
    "        if df[col].isna().sum() > 0.5 * len(df):\n",
    "            continue\n",
    "            \n",
    "        col_results = {}\n",
    "        \n",
    "        # Get non-zero values for analysis\n",
    "        non_zero_mask = df[col] != 0\n",
    "        non_zero_values = df.loc[non_zero_mask, col]\n",
    "        \n",
    "        # If too few non-zero values, skip\n",
    "        if len(non_zero_values) < 10:\n",
    "            continue\n",
    "            \n",
    "        if 'zscore' in methods:\n",
    "            # Z-score method\n",
    "            z_scores = np.abs(stats.zscore(non_zero_values, nan_policy='omit'))\n",
    "            outlier_indices = np.where(z_scores > 3)[0]\n",
    "            outlier_values = non_zero_values.iloc[outlier_indices]\n",
    "            \n",
    "            # Create a mask for the original DataFrame\n",
    "            z_outlier_mask = pd.Series(False, index=df.index)\n",
    "            z_outlier_mask.loc[non_zero_values.index[outlier_indices]] = True\n",
    "            \n",
    "            df_with_flags[f'{col}_zscore_outlier'] = z_outlier_mask\n",
    "            \n",
    "            col_results['zscore'] = {\n",
    "                'indices': non_zero_values.index[outlier_indices].tolist(),\n",
    "                'values': outlier_values.tolist(),\n",
    "                'count': len(outlier_indices),\n",
    "                'percent': (len(outlier_indices) / len(non_zero_values)) * 100\n",
    "            }\n",
    "        \n",
    "        if 'iqr' in methods:\n",
    "            # IQR method\n",
    "            Q1 = non_zero_values.quantile(0.25)\n",
    "            Q3 = non_zero_values.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            iqr_outliers = non_zero_values[(non_zero_values < lower_bound) | (non_zero_values > upper_bound)]\n",
    "            \n",
    "            # Create a mask for the original DataFrame\n",
    "            iqr_outlier_mask = pd.Series(False, index=df.index)\n",
    "            iqr_outlier_mask.loc[iqr_outliers.index] = True\n",
    "            \n",
    "            df_with_flags[f'{col}_iqr_outlier'] = iqr_outlier_mask\n",
    "            \n",
    "            col_results['iqr'] = {\n",
    "                'indices': iqr_outliers.index.tolist(),\n",
    "                'values': iqr_outliers.tolist(),\n",
    "                'count': len(iqr_outliers),\n",
    "                'percent': (len(iqr_outliers) / len(non_zero_values)) * 100,\n",
    "                'bounds': (lower_bound, upper_bound)\n",
    "            }\n",
    "        \n",
    "        if 'percentile' in methods:\n",
    "            # Percentile method\n",
    "            lower = non_zero_values.quantile(0.01)\n",
    "            upper = non_zero_values.quantile(0.99)\n",
    "            \n",
    "            percentile_outliers = non_zero_values[(non_zero_values < lower) | (non_zero_values > upper)]\n",
    "            \n",
    "            # Create a mask for the original DataFrame\n",
    "            percentile_outlier_mask = pd.Series(False, index=df.index)\n",
    "            percentile_outlier_mask.loc[percentile_outliers.index] = True\n",
    "            \n",
    "            df_with_flags[f'{col}_percentile_outlier'] = percentile_outlier_mask\n",
    "            \n",
    "            col_results['percentile'] = {\n",
    "                'indices': percentile_outliers.index.tolist(),\n",
    "                'values': percentile_outliers.tolist(),\n",
    "                'count': len(percentile_outliers),\n",
    "                'percent': (len(percentile_outliers) / len(non_zero_values)) * 100,\n",
    "                'bounds': (lower, upper)\n",
    "            }\n",
    "            \n",
    "        if 'dbscan' in methods or 'isolation_forest' in methods:\n",
    "            try:\n",
    "                from sklearn.ensemble import IsolationForest\n",
    "                from sklearn.cluster import DBSCAN\n",
    "                \n",
    "                # Reshape for sklearn\n",
    "                X = non_zero_values.values.reshape(-1, 1)\n",
    "                \n",
    "                if 'dbscan' in methods:\n",
    "                    # DBSCAN method\n",
    "                    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "                    clusters = dbscan.fit_predict(X)\n",
    "                    dbscan_outliers = non_zero_values[clusters == -1]\n",
    "                    \n",
    "                    # Create a mask for the original DataFrame\n",
    "                    dbscan_outlier_mask = pd.Series(False, index=df.index)\n",
    "                    dbscan_outlier_mask.loc[dbscan_outliers.index] = True\n",
    "                    \n",
    "                    df_with_flags[f'{col}_dbscan_outlier'] = dbscan_outlier_mask\n",
    "                    \n",
    "                    col_results['dbscan'] = {\n",
    "                        'indices': dbscan_outliers.index.tolist(),\n",
    "                        'values': dbscan_outliers.tolist(),\n",
    "                        'count': len(dbscan_outliers),\n",
    "                        'percent': (len(dbscan_outliers) / len(non_zero_values)) * 100\n",
    "                    }\n",
    "                \n",
    "                if 'isolation_forest' in methods:\n",
    "                    # Isolation Forest method\n",
    "                    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "                    yhat = iso_forest.fit_predict(X)\n",
    "                    iso_outliers = non_zero_values[yhat == -1]\n",
    "                    \n",
    "                    # Create a mask for the original DataFrame\n",
    "                    iso_outlier_mask = pd.Series(False, index=df.index)\n",
    "                    iso_outlier_mask.loc[iso_outliers.index] = True\n",
    "                    \n",
    "                    df_with_flags[f'{col}_isolation_forest_outlier'] = iso_outlier_mask\n",
    "                    \n",
    "                    col_results['isolation_forest'] = {\n",
    "                        'indices': iso_outliers.index.tolist(),\n",
    "                        'values': iso_outliers.tolist(),\n",
    "                        'count': len(iso_outliers),\n",
    "                        'percent': (len(iso_outliers) / len(non_zero_values)) * 100\n",
    "                    }\n",
    "            except ImportError:\n",
    "                print(\"scikit-learn is required for DBSCAN and Isolation Forest methods\")\n",
    "        \n",
    "        results[col] = col_results\n",
    "    \n",
    "    return results, df_with_flags\n",
    "\n",
    "def visualize_outliers(df, column, methods=None):\n",
    "    \"\"\"\n",
    "    Visualize outliers in a specific column using boxplots and histograms\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame containing the data\n",
    "    column : str\n",
    "        Column name to visualize\n",
    "    methods : list or None\n",
    "        List of outlier detection methods to highlight in the visualization\n",
    "    \"\"\"\n",
    "    if methods is None:\n",
    "        methods = ['zscore', 'iqr']\n",
    "    \n",
    "    # Create a figure with 2 subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Filter out zeros for better visualization\n",
    "    non_zero_data = df[df[column] > 0][column]\n",
    "    \n",
    "    # Boxplot\n",
    "    sns.boxplot(y=non_zero_data, ax=ax1)\n",
    "    ax1.set_title(f'Boxplot of {column} (Non-Zero Values)')\n",
    "    \n",
    "    # Histogram with KDE\n",
    "    sns.histplot(non_zero_data, kde=True, ax=ax2)\n",
    "    ax2.set_title(f'Distribution of {column} (Non-Zero Values)')\n",
    "    \n",
    "    # Highlight outliers if method flags are available in the DataFrame\n",
    "    for method in methods:\n",
    "        outlier_col = f'{column}_{method}_outlier'\n",
    "        if outlier_col in df.columns:\n",
    "            outlier_values = df[df[outlier_col]][column]\n",
    "            if not outlier_values.empty:\n",
    "                ax1.scatter(\n",
    "                    x=np.zeros(len(outlier_values)), \n",
    "                    y=outlier_values, \n",
    "                    color='red', \n",
    "                    label=f'{method} outliers', \n",
    "                    alpha=0.5\n",
    "                )\n",
    "                \n",
    "                # Add vertical lines on the histogram\n",
    "                for val in outlier_values:\n",
    "                    ax2.axvline(x=val, color='red', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    ax1.legend()\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def get_outlier_summary(results):\n",
    "    \"\"\"\n",
    "    Create a summary DataFrame from the outlier detection results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Results dictionary from detect_outliers function\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame: Summary of outliers for each column and method\n",
    "    \"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    for col, col_results in results.items():\n",
    "        for method, method_results in col_results.items():\n",
    "            summary_data.append({\n",
    "                'Column': col,\n",
    "                'Method': method,\n",
    "                'Outlier Count': method_results['count'],\n",
    "                'Outlier Percentage': method_results['percent'],\n",
    "                'Boundaries': method_results.get('bounds', None)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "def apply_outlier_treatment(df, column, method='iqr', treatment='cap', fill_value=None):\n",
    "    \"\"\"\n",
    "    Apply treatment to outliers in a specific column\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame containing the data\n",
    "    column : str\n",
    "        Column name to treat outliers in\n",
    "    method : str\n",
    "        Method used to detect outliers ('zscore', 'iqr', 'percentile', 'dbscan', 'isolation_forest')\n",
    "    treatment : str\n",
    "        Treatment method:\n",
    "        - 'cap': Cap outliers at the boundaries\n",
    "        - 'remove': Set outliers to NaN\n",
    "        - 'fill': Fill outliers with a specific value\n",
    "        - 'median': Fill outliers with the median\n",
    "        - 'mean': Fill outliers with the mean\n",
    "    fill_value : any\n",
    "        Value to use when treatment is 'fill'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame: DataFrame with treated outliers\n",
    "    \"\"\"\n",
    "    df_treated = df.copy()\n",
    "    outlier_col = f'{column}_{method}_outlier'\n",
    "    \n",
    "    if outlier_col not in df.columns:\n",
    "        print(f\"Column {outlier_col} not found. Run detect_outliers first.\")\n",
    "        return df\n",
    "    \n",
    "    # Non-zero values for calculating bounds\n",
    "    non_zero_mask = df[column] != 0\n",
    "    non_zero_values = df.loc[non_zero_mask, column]\n",
    "    \n",
    "    # Get bounds based on method\n",
    "    if method == 'zscore':\n",
    "        z_scores = np.abs(stats.zscore(non_zero_values, nan_policy='omit'))\n",
    "        outlier_indices = np.where(z_scores > 3)[0]\n",
    "        lower_bound = non_zero_values.loc[~non_zero_values.index.isin(non_zero_values.index[outlier_indices])].min()\n",
    "        upper_bound = non_zero_values.loc[~non_zero_values.index.isin(non_zero_values.index[outlier_indices])].max()\n",
    "    elif method == 'iqr':\n",
    "        Q1 = non_zero_values.quantile(0.25)\n",
    "        Q3 = non_zero_values.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "    elif method == 'percentile':\n",
    "        lower_bound = non_zero_values.quantile(0.01)\n",
    "        upper_bound = non_zero_values.quantile(0.99)\n",
    "    else:\n",
    "        # For DBSCAN and Isolation Forest, use non-outlier min/max\n",
    "        outlier_mask = df[outlier_col]\n",
    "        non_outliers = df.loc[~outlier_mask & non_zero_mask, column]\n",
    "        lower_bound = non_outliers.min()\n",
    "        upper_bound = non_outliers.max()\n",
    "    \n",
    "    # Get outlier indices\n",
    "    outlier_mask = df[outlier_col]\n",
    "    \n",
    "    if treatment == 'cap':\n",
    "        # Cap outliers at the boundaries\n",
    "        df_treated.loc[outlier_mask & (df[column] < lower_bound), column] = lower_bound\n",
    "        df_treated.loc[outlier_mask & (df[column] > upper_bound), column] = upper_bound\n",
    "    elif treatment == 'remove':\n",
    "        # Set outliers to NaN\n",
    "        df_treated.loc[outlier_mask, column] = np.nan\n",
    "    elif treatment == 'fill':\n",
    "        # Fill outliers with specified value\n",
    "        df_treated.loc[outlier_mask, column] = fill_value\n",
    "    elif treatment == 'median':\n",
    "        # Fill outliers with median of non-outliers\n",
    "        median_value = df.loc[~outlier_mask & non_zero_mask, column].median()\n",
    "        df_treated.loc[outlier_mask, column] = median_value\n",
    "    elif treatment == 'mean':\n",
    "        # Fill outliers with mean of non-outliers\n",
    "        mean_value = df.loc[~outlier_mask & non_zero_mask, column].mean()\n",
    "        df_treated.loc[outlier_mask, column] = mean_value\n",
    "    \n",
    "    return df_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assume df_model_training is your dataframe with all the preprocessing already done\n",
    "# as shown in your code snippet\n",
    "\n",
    "# 1. Select features for clustering\n",
    "features = ['Manufacturer', '5G', '4G', 'resolution width', 'resolution height', \n",
    "            'Diagonala', 'Numar nuclee', 'Memorie RAM', 'Incarcare Wireless', \n",
    "            'Capacitate Baterie', 'Dual SIM']\n",
    "\n",
    "# 2. Extract features for clustering\n",
    "X = df_model_training[features].copy()\n",
    "\n",
    "# 3. Standardize the data (important for K-means)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 4. Determine optimal number of clusters using the Elbow method and Silhouette score\n",
    "def find_optimal_clusters(X, max_k=10):\n",
    "    \"\"\"Find the optimal number of clusters using both Elbow method and Silhouette score.\"\"\"\n",
    "    inertia_values = []\n",
    "    silhouette_scores = []\n",
    "    k_range = range(2, max_k+1)\n",
    "    \n",
    "    for k in k_range:\n",
    "        # K-means clustering\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Inertia (for Elbow method)\n",
    "        inertia_values.append(kmeans.inertia_)\n",
    "        \n",
    "        # Silhouette score\n",
    "        sil_score = silhouette_score(X, cluster_labels)\n",
    "        silhouette_scores.append(sil_score)\n",
    "        \n",
    "        print(f'K = {k}, Inertia = {kmeans.inertia_:.2f}, Silhouette Score = {sil_score:.3f}')\n",
    "    \n",
    "    # Plot the Elbow method\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(k_range, inertia_values, 'bo-')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(k_range, silhouette_scores, 'ro-')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Method')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Best k based on silhouette score\n",
    "    best_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
    "    \n",
    "    return best_k_silhouette, inertia_values, silhouette_scores\n",
    "\n",
    "# Find optimal number of clusters\n",
    "best_k, inertia_values, silhouette_scores = find_optimal_clusters(X_scaled)\n",
    "print(f\"\\nOptimal number of clusters based on silhouette score: {best_k}\")\n",
    "\n",
    "# 5. Apply K-means with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "df_model_training['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# 6. Visualize clusters using PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df_model_training['Cluster'], \n",
    "                      cmap='viridis', alpha=0.8, s=50)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title('Cluster Visualization using PCA')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Analyze cluster characteristics\n",
    "cluster_stats = df_model_training.groupby('Cluster')[features].agg(['mean', 'median', 'std'])\n",
    "print(\"\\nCluster Statistics:\")\n",
    "print(cluster_stats)\n",
    "\n",
    "# 8. Analyze feature importance for each cluster\n",
    "def analyze_cluster_importance(X_scaled, cluster_labels, feature_names, n_top_features=5):\n",
    "    \"\"\"Identify the most important features for each cluster.\"\"\"\n",
    "    n_clusters = len(np.unique(cluster_labels))\n",
    "    centers = kmeans.cluster_centers_\n",
    "    \n",
    "    # Calculate the overall mean across all data\n",
    "    overall_mean = np.mean(X_scaled, axis=0)\n",
    "    \n",
    "    # For each cluster, find features that deviate the most from the overall mean\n",
    "    cluster_feature_importance = {}\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        # Calculate how much each feature in this cluster deviates from the overall mean\n",
    "        deviation = np.abs(centers[i] - overall_mean)\n",
    "        \n",
    "        # Sort features by deviation\n",
    "        sorted_features = np.argsort(-deviation)\n",
    "        \n",
    "        # Get the top N most important features\n",
    "        top_features = [(feature_names[idx], deviation[idx]) for idx in sorted_features[:n_top_features]]\n",
    "        \n",
    "        cluster_feature_importance[i] = top_features\n",
    "    \n",
    "    return cluster_feature_importance\n",
    "\n",
    "# Analyze feature importance\n",
    "importance = analyze_cluster_importance(X_scaled, df_model_training['Cluster'], features)\n",
    "\n",
    "# Print the most important features for each cluster\n",
    "print(\"\\nMost distinctive features for each cluster:\")\n",
    "for cluster, features_list in importance.items():\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    for feature, score in features_list:\n",
    "        print(f\"  - {feature}: {score:.3f}\")\n",
    "\n",
    "# 9. Create a radar chart to visualize cluster characteristics\n",
    "def radar_chart(df, features, cluster_id):\n",
    "    \"\"\"Create a radar chart for a specific cluster compared to overall average.\"\"\"\n",
    "    cluster_data = df[df['Cluster'] == cluster_id][features].mean()\n",
    "    all_data = df[features].mean()\n",
    "    \n",
    "    # Normalize data for radar chart\n",
    "    max_values = df[features].max()\n",
    "    min_values = df[features].min()\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    denominator = max_values - min_values\n",
    "    denominator = denominator.replace(0, 1)  # Replace zeros with ones\n",
    "    \n",
    "    cluster_data_norm = (cluster_data - min_values) / denominator\n",
    "    all_data_norm = (all_data - min_values) / denominator\n",
    "    \n",
    "    # Set up the radar chart\n",
    "    categories = features\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Create angles for each feature\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Convert to list for plotting\n",
    "    cluster_values = cluster_data_norm.tolist()\n",
    "    cluster_values += cluster_values[:1]  # Close the loop\n",
    "    \n",
    "    all_values = all_data_norm.tolist()\n",
    "    all_values += all_values[:1]  # Close the loop\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # Draw the cluster line\n",
    "    ax.plot(angles, cluster_values, 'o-', linewidth=2, label=f'Cluster {cluster_id}')\n",
    "    # Draw the average line\n",
    "    ax.plot(angles, all_values, 'o-', linewidth=2, label='Overall Average')\n",
    "    \n",
    "    # Fill area\n",
    "    ax.fill(angles, cluster_values, alpha=0.25)\n",
    "    \n",
    "    # Set category labels\n",
    "    plt.xticks(angles[:-1], categories, size=12)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    \n",
    "    plt.title(f'Characteristics of Cluster {cluster_id}', size=15)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create radar charts for each cluster\n",
    "for cluster_id in range(best_k):\n",
    "    radar_chart(df_model_training, features, cluster_id)\n",
    "    plt.show()\n",
    "\n",
    "# 10. Export the results\n",
    "# Add cluster labels to the original dataframe\n",
    "print(\"\\nSample of products with cluster assignments:\")\n",
    "print(df_model_training[['Cluster'] + features].head(10))\n",
    "\n",
    "# Optional: Save the clustering model for future use\n",
    "import joblib\n",
    "joblib.dump(kmeans, 'product_clusters_model.pkl')\n",
    "joblib.dump(scaler, 'product_clusters_scaler.pkl')\n",
    "\n",
    "print(\"\\nClustering complete. Model and scaler saved for future use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_training\n",
    "# Get separate dataframes for each cluster\n",
    "cluster_dataframes = {}\n",
    "for cluster_id in range(best_k):\n",
    "    cluster_dataframes[cluster_id] = df_model_training[df_model_training['Cluster'] == cluster_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cluster_dataframes[9]['price']\n",
    "cluster_dataframes[9]\n",
    "df_pt_llm = cluster_dataframes[9]\n",
    "df_pt_llm.drop('price', axis=1, inplace=True)\n",
    "df_pt_llm.drop('Cluster', axis=1, inplace=True)\n",
    "df_pt_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: If df_model_training and y have different lengths\n",
    "# Make sure they have the same index and align them\n",
    "df_model_training = df_pt_llm.loc[y.index]  # If y is a Series\n",
    "# OR\n",
    "y = y[df_model_training.index]  # Adjust y to match df_model_training\n",
    "\n",
    "# Approach 2: If using the wrong dataframe for feature names\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(df_pt_llm, y)\n",
    "\n",
    "# Get feature importance using the correct columns\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': df_model_training.columns,  # Use the same dataframe you used for training\n",
    "    'Importance': rf.feature_importances_\n",
    "})\n",
    "print(rf_importance.sort_values('Importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# For regression models\n",
    "predictions = rf.predict(df_model_training)\n",
    "print(f\"R² Score: {r2_score(y, predictions)}\")\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y, predictions)}\")\n",
    "print(f\"Root Mean Squared Error: {mean_squared_error(y, predictions, squared=False)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y, predictions)}\")\n",
    "\n",
    "# Cross-validation (more robust evaluation)\n",
    "cv_scores = cross_val_score(rf, df_model_training, y, cv=5, scoring='r2')\n",
    "print(f\"Cross-validation R² scores: {cv_scores}\")\n",
    "print(f\"Mean CV R² score: {cv_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
